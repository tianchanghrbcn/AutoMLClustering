@ARTICLE{10.3389/fncom.2019.00031,

AUTHOR={Alashwal, Hany  and El Halaby, Mohamed  and Crouse, Jacob J.  and Abdalla, Areeg  and Moustafa, Ahmed A. },

TITLE={The Application of Unsupervised Clustering Methods to Alzheimer’s Disease},

JOURNAL={Frontiers in Computational Neuroscience},

VOLUME={13},

YEAR={2019},

URL={https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2019.00031},

DOI={10.3389/fncom.2019.00031},

ISSN={1662-5188},

ABSTRACT={<p>Clustering is a powerful machine learning tool for detecting structures in datasets. In the medical field, clustering has been proven to be a powerful tool for discovering patterns and structure in labeled and unlabeled datasets. Unlike supervised methods, clustering is an unsupervised method that works on datasets in which there is no outcome (target) variable nor is anything known about the relationship between the observations, that is, unlabeled data. In this paper, we focus on studying and reviewing clustering methods that have been applied to datasets of neurological diseases, especially Alzheimer’s disease (AD). The aim is to provide insights into which clustering technique is more suitable for partitioning patients of AD based on their similarity. This is important as clustering algorithms can find patterns across patients that are difficult for medical practitioners to find. We further discuss the implications of the use of clustering algorithms in the treatment of AD. We found that clustering analysis can point to several features that underlie the conversion from early-stage AD to advanced AD. Furthermore, future work can apply semi-clustering algorithms on AD datasets, which will enhance clusters by including additional information.</p>}}

@ARTICLE{7442571,
  author={Bi, Wenjie and Cai, Meili and Liu, Mengqi and Li, Guo},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={A Big Data Clustering Algorithm for Mitigating the Risk of Customer Churn}, 
  year={2016},
  volume={12},
  number={3},
  pages={1270-1281},
  keywords={Big data;Clustering algorithms;Algorithm design and analysis;Companies;Semantics;Prediction algorithms;Clustering methods;Axiomatic Fuzzy Sets;MapReduce;Semantic Driven Subtractive Clustering Method;Subtractive Clustering Method;Axiomatic fuzzy sets (AFSs);mapreduce;semantic-driven subtractive clustering method (SDSCM);subtractive clustering method (SCM)},
  doi={10.1109/TII.2016.2547584}}

@article{Li2023,
  author = {Li, Meng and Liu, Jiqiang and Yang, Yeping},
  title = {Financial Data Quality Evaluation Method Based on Multiple Linear Regression},
  journal = {Future Internet},
  volume = {15},
  year = {2023},
  number = {10},
  article-number = {338},
  url = {https://www.mdpi.com/1999-5903/15/10/338},
  issn = {1999-5903},
  doi = {10.3390/fi15100338}
}

@article{Abdulhameed2024,
  author = {Zaki Abdulhameed, Tiba and Yousif, Suhad A. and Samawi, Venus W. and Imad Al-Shaikhli, Hasnaa},
  journal = {IEEE Access}, 
  title = {SS-DBSCAN: Semi-Supervised Density-Based Spatial Clustering of Applications With Noise for Meaningful Clustering in Diverse Density Data}, 
  year = {2024},
  volume = {12},
  pages = {131507-131520},
  keywords = {Clustering algorithms, Noise measurement, Measurement, Complexity theory, Classification algorithms, Wireless sensor networks, Semisupervised learning, Unsupervised learning, Text categorization, Clustering, DBSCAN, Semi-supervised clustering, Unsupervised classification, Word classification},
  doi = {10.1109/ACCESS.2024.3457587}
}

@article{Bahmani2021ToTune,
  author = {Mohamadjavad Bahmani and Radwa El Shawi and Nshan Potikyan and Sherif Sakr},
  title = {To tune or not to tune? An Approach for Recommending Important Hyperparameters},
  journal = {arXiv preprint arXiv:2108.13066},
  year = {2021},
  url = {https://doi.org/10.48550/arXiv.2108.13066},
  note = {Presented at The Fifth International Workshop on Automation in Machine Learning, in conjunction with KDD 2021 Conference}
}

@article{Bandyapadhyay2024,
  author = {Sayan Bandyapadhyay and Zachary Friggstad and Ramin Mousavi},
  title = {Parameterized Approximation Algorithms and Lower Bounds for k-Center Clustering and Variants},
  journal = {Algorithmica},
  volume = {86},
  number = {8},
  pages = {2557--2574},
  year = {2024},
  month = {August},
  doi = {10.1007/s00453-024-01236-1},
  url = {https://doi.org/10.1007/s00453-024-01236-1},
  issn = {1432-0541}
}

@article{Zhang2025,
  title = {The distance and entropy measures-based intuitionistic fuzzy C-means and similarity matrix clustering algorithms and their applications},
  journal = {Applied Soft Computing},
  volume = {169},
  pages = {112581},
  year = {2025},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2024.112581},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494624013553},
  author = {Yueyue Zhang and Han-Liang Huang},
  keywords = {Mixed correlation coefficient, Distance measures, Entropy, Dynamic time warping, Clustering algorithms}
}

@article{Lin2024,
  title = {Data-driven solutions: Uncovering the hidden potential of big data technologies in building low-carbon cities},
  journal = {Computers \& Industrial Engineering},
  volume = {197},
  pages = {110543},
  year = {2024},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2024.110543},
  url = {https://www.sciencedirect.com/science/article/pii/S0360835224006648},
  author = {Zihao Lin},
  keywords = {National Big Data Comprehensive Pilot Zone, Carbon emission, Green innovation, Internet}
}

@article{Atif2024,
  author = {M. Atif and M. Farooq and M. Shafiq and others},
  title = {Uncovering the impact of outliers on clusters’ evolution in temporal data-sets: an empirical analysis},
  journal = {Scientific Reports},
  volume = {14},
  pages = {30674},
  year = {2024},
  doi = {10.1038/s41598-024-75928-7},
  url = {https://doi.org/10.1038/s41598-024-75928-7}
}

@article{Sloutsky2012,
  author = {Sloutsky, Roman and Jimenez, Nicolas and Swamidass, S. Joshua and Naegle, Kristen M.},
  title = {Accounting for noise when clustering biological data},
  journal = {Briefings in Bioinformatics},
  volume = {14},
  number = {4},
  pages = {423-436},
  year = {2012},
  month = {10},
  abstract = {Clustering is a powerful and commonly used technique that organizes and elucidates the structure of biological data. Clustering data from gene expression, metabolomics and proteomics experiments has proven to be useful at deriving a variety of insights, such as the shared regulation or function of biochemical components within networks. However, experimental measurements of biological processes are subject to substantial noise—stemming from both technical and biological variability—and most clustering algorithms are sensitive to this noise. In this article, we explore several methods of accounting for noise when analyzing biological data sets through clustering. Using a toy data set and two different case studies—gene expression and protein phosphorylation—we demonstrate the sensitivity of clustering algorithms to noise. Several methods of accounting for this noise can be used to establish when clustering results can be trusted. These methods span a range of assumptions about the statistical properties of the noise and can therefore be applied to virtually any biological data source.},
  issn = {1467-5463},
  doi = {10.1093/bib/bbs057},
  url = {https://doi.org/10.1093/bib/bbs057},
  eprint = {https://academic.oup.com/bib/article-pdf/14/4/423/478640/bbs057.pdf}
}

@article{Guo2024,
  author    = {Guo, H. and Yin, H. and Song, S. and others},
  title     = {Application of density clustering with noise combined with particle swarm optimization in UWB indoor positioning},
  journal   = {Scientific Reports},
  volume    = {14},
  pages     = {13121},
  year      = {2024},
  doi       = {10.1038/s41598-024-63358-4},
  url       = {https://doi.org/10.1038/s41598-024-63358-4}
}

@Article{app112311202,
AUTHOR = {Ran, Xiaojuan and Zhou, Xiangbing and Lei, Mu and Tepsan, Worawit and Deng, Wu},
TITLE = {A Novel K-Means Clustering Algorithm with a Noise Algorithm for Capturing Urban Hotspots},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {23},
ARTICLE-NUMBER = {11202},
URL = {https://www.mdpi.com/2076-3417/11/23/11202},
ISSN = {2076-3417},
ABSTRACT = {With the development of cities, urban congestion is nearly an unavoidable problem for almost every large-scale city. Road planning is an effective means to alleviate urban congestion, which is a classical non-deterministic polynomial time (NP) hard problem, and has become an important research hotspot in recent years. A K-means clustering algorithm is an iterative clustering analysis algorithm that has been regarded as an effective means to solve urban road planning problems by scholars for the past several decades; however, it is very difficult to determine the number of clusters and sensitively initialize the center cluster. In order to solve these problems, a novel K-means clustering algorithm based on a noise algorithm is developed to capture urban hotspots in this paper. The noise algorithm is employed to randomly enhance the attribution of data points and output results of clustering by adding noise judgment in order to automatically obtain the number of clusters for the given data and initialize the center cluster. Four unsupervised evaluation indexes, namely, DB, PBM, SC, and SSE, are directly used to evaluate and analyze the clustering results, and a nonparametric Wilcoxon statistical analysis method is employed to verify the distribution states and differences between clustering results. Finally, five taxi GPS datasets from Aracaju (Brazil), San Francisco (USA), Rome (Italy), Chongqing (China), and Beijing (China) are selected to test and verify the effectiveness of the proposed noise K-means clustering algorithm by comparing the algorithm with fuzzy C-means, K-means, and K-means plus approaches. The compared experiment results show that the noise algorithm can reasonably obtain the number of clusters and initialize the center cluster, and the proposed noise K-means clustering algorithm demonstrates better clustering performance and accurately obtains clustering results, as well as effectively capturing urban hotspots.},
DOI = {10.3390/app112311202}
}

@article{Ni2023,
  author = {Wei Ni and Xiaoye Miao and Xiangyu Zhao and Yangyang Wu and Jianwei Yin},
  title = {Automatic Data Repair: Are We Ready to Deploy?},
  journal = {arXiv preprint arXiv:2310.00711},
  year = {2023},
  archivePrefix = {arXiv},
  primaryClass = {cs.DB},
  url = {https://doi.org/10.48550/arXiv.2310.00711},
  note = {14 pages, 51 figures}
}

@INPROCEEDINGS{9458702,
  author={Li, Peng and Rao, Xi and Blase, Jennifer and Zhang, Yue and Chu, Xu and Zhang, Ce},
  booktitle={2021 IEEE 37th International Conference on Data Engineering (ICDE)}, 
  title={CleanML: A Study for Evaluating the Impact of Data Cleaning on ML Classification Tasks}, 
  year={2021},
  volume={},
  number={},
  pages={13-24},
  keywords={Training;Systematics;Machine learning algorithms;Machine learning;Cleaning;Data models;Classification algorithms;Data Cleaning;Machine Learning},
  doi={10.1109/ICDE51399.2021.00009}}

@article{Rekatsinas2017,
  author = {Theodoros Rekatsinas and Xu Chu and Ihab F. Ilyas and Christopher Ré},
  title = {HoloClean: Holistic Data Repairs with Probabilistic Inference},
  journal = {arXiv preprint arXiv:1702.00820},
  year = {2017},
  archivePrefix = {arXiv},
  primaryClass = {cs.DB},
  url = {https://doi.org/10.48550/arXiv.1702.00820}
}

@inproceedings{10.1145/2723372.2749431,
author = {Chu, Xu and Morcos, John and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Tang, Nan and Ye, Yin},
title = {KATARA: A Data Cleaning System Powered by Knowledge Bases and Crowdsourcing},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2749431},
doi = {10.1145/2723372.2749431},
abstract = {Classical approaches to clean data have relied on using integrity constraints, statistics, or machine learning. These approaches are known to be limited in the cleaning accuracy, which can usually be improved by consulting master data and involving experts to resolve ambiguity. The advent of knowledge bases KBs both general-purpose and within enterprises, and crowdsourcing marketplaces are providing yet more opportunities to achieve higher accuracy at a larger scale. We propose KATARA, a knowledge base and crowd powered data cleaning system that, given a table, a KB, and a crowd, interprets table semantics to align it with the KB, identifies correct and incorrect data, and generates top-k possible repairs for incorrect data. Experiments show that KATARA can be applied to various datasets and KBs, and can efficiently annotate data and suggest possible repairs.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1261},
numpages = {15},
keywords = {knowledge base, data quality, data cleaning, crowdsourcing},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}

@inproceedings{10.1145/2463676.2465327,
author = {Dallachiesa, Michele and Ebaid, Amr and Eldawy, Ahmed and Elmagarmid, Ahmed and Ilyas, Ihab F. and Ouzzani, Mourad and Tang, Nan},
title = {NADEEF: a commodity data cleaning system},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2465327},
doi = {10.1145/2463676.2465327},
abstract = {Despite the increasing importance of data quality and the rich theoretical and practical contributions in all aspects of data cleaning, there is no single end-to-end off-the-shelf solution to (semi-)automate the detection and the repairing of violations w.r.t. a set of heterogeneous and ad-hoc quality constraints. In short, there is no commodity platform similar to general purpose DBMSs that can be easily customized and deployed to solve application-specific data quality problems. In this paper, we present NADEEF, an extensible, generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between a programming interface and a core to achieve generality and extensibility. The programming interface allows the users to specify multiple types of data quality rules, which uniformly define what is wrong with the data and (possibly) how to repair it through writing code that implements predefined classes. We show that the programming interface can be used to express many types of data quality rules beyond the well known CFDs (FDs), MDs and ETL rules. Treating user implemented interfaces as black-boxes, the core provides algorithms to detect errors and to clean data. The core is designed in a way to allow cleaning algorithms to cope with multiple rules holistically, i.e. detecting and repairing data errors without differentiating between various types of rules. We showcase two implementations for core repairing algorithms. These two implementations demonstrate the extensibility of our core, which can also be replaced by other user-provided algorithms. Using real-life data, we experimentally verify the generality, extensibility, and effectiveness of our system.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {541–552},
numpages = {12},
keywords = {conditional functional dependency, data cleaning, etl, matching dependency},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{Krishnan2017,
  author = {Sanjay Krishnan and Michael J. Franklin and Ken Goldberg and Eugene Wu},
  title = {BoostClean: Automated Error Detection and Repair for Machine Learning},
  journal = {arXiv preprint arXiv:1711.01299},
  year = {2017},
  archivePrefix = {arXiv},
  primaryClass = {cs.DB},
  url = {https://doi.org/10.48550/arXiv.1711.01299}
}

@inproceedings{10.1145/3357384.3358129,
author = {Neutatz, Felix and Mahdavi, Mohammad and Abedjan, Ziawasch},
title = {ED2: A Case for Active Learning in Error Detection},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358129},
doi = {10.1145/3357384.3358129},
abstract = {State-of-the-art approaches formulate error detection as a semi-supervised classification problem. Recent research suggests that active learning is insufficiently effective for error detection and proposes the usage of neural networks and data augmentation to reduce the number of these user-provided labels. However, we can show that using the appropriate active learning strategy, it is possible to outperform the more complex models that rely on data augmentation. To this end, we propose a multi-classifier approach with two-stage sampling for active learning. This intuitive and neat sampling method chooses the most promising cells across rows and columns for labeling. On three datasets, ED2 achieves state-of-the-art detection accuracy while for large datasets, the required number of user labels is lower by one order of magnitude compared to the state of the art.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2249–2252},
numpages = {4},
keywords = {example-driven error detection, error detection, data quality, active learning},
location = {Beijing, China},
series = {CIKM '19}
}

@INPROCEEDINGS{10346079,
  author={Panchal, Deven and Baran, Isilay and Musgrove, Dan and Lu, David},
  booktitle={2023 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS)}, 
  title={MLOps: Automatic, Zero-Touch and Reusable Machine Learning Training and Serving Pipelines}, 
  year={2023},
  volume={},
  number={},
  pages={175-181},
  keywords={Training;Cloud computing;Forensics;Pipelines;Production;Software;Data models;Acumos;Machine Learning;Deep Learning;MLOps;Platform;ML Pipelines;Microservices;Nifi;Reusable ML;Open Source;AI4EU},
  doi={10.1109/IoTaIS60147.2023.10346079}}

@article{SINGH2024102799,
title = {A comprehensive review of clustering techniques in artificial intelligence for knowledge discovery: Taxonomy, challenges, applications and future prospects},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102799},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102799},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004476},
author = {Jaswinder Singh and Damanpreet Singh},
keywords = {Machine learning, Unsupervised learning, Data clustering, Pattern recognition, Automatic data clustering, Hybrid clustering},
abstract = {Clustering is a set of essential mathematical techniques in artificial intelligence and machine learning for analyzing massive amounts of data generated by applications. Clustering uses data mining approaches to group data points based on their intrinsic characteristics or similarity measures. These measures are essential for data mining tasks such as information retrieval, pattern recognition, classifications, and clustering from the raw data. However, significant efforts are needed to select the appropriate similarity metrics based on the distribution of data points and problem domains. Various clustering approaches and techniques are actively utilized in a variety of disciplines of research, including data science, machine learning, computer science, pattern recognition, computer vision, etc. Some partitional clustering techniques, such as K-Means and density-based clustering, etc., take some sensitive parameters from the user prior to grouping the objects, and variations on these parameters may lead to different results for the same dataset. Some traditional statistical techniques for clustering are unable to find optimal clusters or handle high-dimensional data effectively. The need for a priori requirements of elements, for instance, the number of groups, termination at the local optimum, and expensive computations are some of its limitations that must be solved. To get beyond the aforementioned restrictions, innovative, adaptable, efficient, and hybrid clustering algorithms must be created. This study provides a comprehensive review of the literature on traditional and novel clustering techniques in a cohesive manner, their trending applications in various domains, their summarization, challenges, and future scope. In addition, data clustering embraces various scientific disciplines. Thus, this study will be beneficial and will provide an effective reference point for progressive researchers, analysts, and artificial intelligence professionals to develop novel, flexible, and efficient state-of-the-art clustering techniques.}
}

@article{IKOTUN2023178,
title = {K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data},
journal = {Information Sciences},
volume = {622},
pages = {178-210},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.139},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522014633},
author = {Abiodun M. Ikotun and Absalom E. Ezugwu and Laith Abualigah and Belal Abuhaija and Jia Heming},
keywords = {K-means, K-means variants, Clustering algorithm, Modified k-means, Improved k-means, Perspectives on big data clustering, Big data clustering},
abstract = {Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm's performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.}
}

@article{Ran2023,
  author = {Ran, X. and Xi, Y. and Lu, Y. and others},
  title = {Comprehensive survey on hierarchical clustering algorithms and the recent developments},
  journal = {Artificial Intelligence Review},
  volume = {56},
  pages = {8219--8264},
  year = {2023},
  month = {August},
  doi = {10.1007/s10462-022-10366-3},
  url = {https://doi.org/10.1007/s10462-022-10366-3}
}

@article{10.1145/3299876,
author = {Barton, Tomas and Bruna, Tomas and Kordik, Pavel},
title = {Chameleon 2: An Improved Graph-Based Clustering Algorithm},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3299876},
doi = {10.1145/3299876},
abstract = {Traditional clustering algorithms fail to produce human-like results when confronted with data of variable density, complex distributions, or in the presence of noise. We propose an improved graph-based clustering algorithm called Chameleon 2, which overcomes several drawbacks of state-of-the-art clustering approaches. We modified the internal cluster quality measure and added an extra step to ensure algorithm robustness. Our results reveal a significant positive impact on the clustering quality measured by Normalized Mutual Information on 32 artificial datasets used in the clustering literature. This significant improvement is also confirmed on real-world datasets.The performance of clustering algorithms such as DBSCAN is extremely parameter sensitive, and exhaustive manual parameter tuning is necessary to obtain a meaningful result. All hierarchical clustering methods are very sensitive to cutoff selection, and a human expert is often required to find the true cutoff for each clustering result. We present an automated cutoff selection method that enables the Chameleon 2 algorithm to generate high-quality clustering in autonomous mode.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {10},
numpages = {27},
keywords = {Cluster analysis, clustering, graph clustering, pattern recognition}
}

@article{HORNG2011306,
title = {A novel intrusion detection system based on hierarchical clustering and support vector machines},
journal = {Expert Systems with Applications},
volume = {38},
number = {1},
pages = {306-313},
year = {2011},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2010.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S0957417410005701},
author = {Shi-Jinn Horng and Ming-Yang Su and Yuan-Hsin Chen and Tzong-Wann Kao and Rong-Jian Chen and Jui-Lin Lai and Citra Dwi Perkasa},
keywords = {Network intrusion detection system (NIDS), Support vector machines (SVMs), Hierarchical clustering algorithm, KDD Cup 1999, Network security, Data mining},
abstract = {This study proposed an SVM-based intrusion detection system, which combines a hierarchical clustering algorithm, a simple feature selection procedure, and the SVM technique. The hierarchical clustering algorithm provided the SVM with fewer, abstracted, and higher-qualified training instances that are derived from the KDD Cup 1999 training set. It was able to greatly shorten the training time, but also improve the performance of resultant SVM. The simple feature selection procedure was applied to eliminate unimportant features from the training set so the obtained SVM model could classify the network traffic data more accurately. The famous KDD Cup 1999 dataset was used to evaluate the proposed system. Compared with other intrusion detection systems that are based on the same dataset, this system showed better performance in the detection of DoS and Probe attacks, and the beset performance in overall accuracy.}
}

@article{Sadeghi2021,
  author = {Banafsheh Sadeghi and Rex C. Y. Cheung and Meagan Hanbury},
  title = {Using hierarchical clustering analysis to evaluate COVID-19 pandemic preparedness and performance in 180 countries in 2020},
  journal = {BMJ Open},
  volume = {11},
  number = {11},
  pages = {e049844},
  year = {2021},
  month = {November},
  doi = {10.1136/bmjopen-2021-049844},
  url = {https://doi.org/10.1136/bmjopen-2021-049844},
  pmid = {34753756},
  pmc = {PMC8578186},
  publisher = {BMJ Open}
}

@article{Cai2016,
  author = {Fan Cai and Nhien-An Le-Khac and Tahar Kechadi},
  title = {Clustering Approaches for Financial Data Analysis: a Survey},
  journal = {arXiv preprint arXiv:1609.08520},
  year = {2016},
  archivePrefix = {arXiv},
  primaryClass = {q-fin.GN},
  url = {https://doi.org/10.48550/arXiv.1609.08520}
}

@article{Blumenberg2020,
  author = {Lili Blumenberg and Kelly V. Ruggles},
  title = {Hypercluster: a flexible tool for parallelized unsupervised clustering optimization},
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {428},
  year = {2020},
  month = {September},
  doi = {10.1186/s12859-020-03774-1},
  url = {https://doi.org/10.1186/s12859-020-03774-1},
  issn = {1471-2105}
}

@article{BOLANOSMARTINEZ2024102164,
title = {Clustering pipeline for vehicle behavior in smart villages},
journal = {Information Fusion},
volume = {104},
pages = {102164},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102164},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004803},
author = {Daniel Bolaños-Martinez and Maria Bermudez-Edo and Jose Luis Garrido},
keywords = {Internet of Things (IoT), Sensors, Clustering, Smart villages, Explainability},
abstract = {Smart cities and villages present a plethora of opportunities for fusing and managing multi-source data. However, in the analysis of mobility patterns, the use of only one data source (i.e., road sensors) without considering other contextual data sources, limits the understanding of the process. To address this gap, we propose a pipeline that integrates multiple data sources, providing valuable information for pattern extraction, mainly based on vehicle mobility behavior and provenance. Our research also highlights the critical role of selecting the appropriate normalization algorithm to scale input features from heterogeneous data sources, which has not received sufficient attention in the literature. We conducted our analysis using data from four License Plate Recognition (LPR) cameras, spanning nine months, and incorporating several databases that include provenance, gross income, and holiday information, resulting in a dataset of over 50,000 vehicles. Using this data and our clustering pipeline, we identified various traffic patterns among residents and visitors in a rural touristic area. Our findings assist data analysts in choosing algorithms for analyzing heterogeneous datasets. Moreover, policymakers could use our results to adjust the resources, such as new parking zones.}
}

@article{SALEHIN202452,
title = {AutoML: A systematic review on automated machine learning with neural architecture search},
journal = {Journal of Information and Intelligence},
volume = {2},
number = {1},
pages = {52-81},
year = {2024},
issn = {2949-7159},
doi = {https://doi.org/10.1016/j.jiixd.2023.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2949715923000604},
author = {Imrus Salehin and Md. Shamiul Islam and Pritom Saha and S.M. Noman and Azra Tuni and Md. Mehedi Hasan and Md. Abu Baten},
keywords = {AutoML, Neural architecture search, Advance machine learning, Search space, Hyperparameter optimization},
abstract = {AutoML (Automated Machine Learning) is an emerging field that aims to automate the process of building machine learning models. AutoML emerged to increase productivity and efficiency by automating as much as possible the inefficient work that occurs while repeating this process whenever machine learning is applied. In particular, research has been conducted for a long time on technologies that can effectively develop high-quality models by minimizing the intervention of model developers in the process from data preprocessing to algorithm selection and tuning. In this semantic review research, we summarize the data processing requirements for AutoML approaches and provide a detailed explanation. We place greater emphasis on neural architecture search (NAS) as it currently represents a highly popular sub-topic within the field of AutoML. NAS methods use machine learning algorithms to search through a large space of possible architectures and find the one that performs best on a given task. We provide a summary of the performance achieved by representative NAS algorithms on the CIFAR-10, CIFAR-100, ImageNet and well-known benchmark datasets. Additionally, we delve into several noteworthy research directions in NAS methods including one/two-stage NAS, one-shot NAS and joint hyperparameter with architecture optimization. We discussed how the search space size and complexity in NAS can vary depending on the specific problem being addressed. To conclude, we examine several open problems (SOTA problems) within current AutoML methods that assure further investigation in future research.}
}